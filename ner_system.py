"""
–ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ NER —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≥—Ä–∞—Ñ–∏–∫–æ–≤
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup
from datasets import load_dataset
import numpy as np
import pandas as pd
from collections import defaultdict
from seqeval.metrics import classification_report, f1_score, precision_score, recall_score
import random
from tqdm import tqdm
import matplotlib
# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±—ç–∫–µ–Ω–¥ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –±–µ–∑ –¥–∏—Å–ø–ª–µ—è
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import json
import warnings
import argparse
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import time

warnings.filterwarnings('ignore')

# –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å BERT –¥–ª—è NER
class SimpleBertForNER(nn.Module):
    """–ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å BERT –¥–ª—è NER"""
    
    def __init__(self, model_name, num_tags, dropout_prob=0.1):
        super().__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout_prob)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_tags)
    
    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        sequence_output = outputs.last_hidden_state
        sequence_output = self.dropout(sequence_output)
        logits = self.classifier(sequence_output)
        
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            # –í—ã—á–∏—Å–ª—è–µ–º loss —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
            active_loss = attention_mask.view(-1) == 1
            active_logits = logits.view(-1, logits.shape[-1])[active_loss]
            active_labels = labels.view(-1)[active_loss]
            loss = loss_fct(active_logits, active_labels)
            return loss, logits
        
        return logits

# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
class FastNERDataset(Dataset):
    def __init__(self, dataset_split, tokenizer, max_length=128, num_workers=4, debug=False, max_samples=2000):
        self.dataset = dataset_split
        if debug:
            max_samples = min(max_samples, len(self.dataset))
            self.dataset = self.dataset.select(range(max_samples))
        
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å—é
        print(f"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(self.dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤...")
        self.data = self._preprocess_parallel(num_workers)
    
    def _preprocess_parallel(self, num_workers):
        """–ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        results = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ThreadPoolExecutor –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = []
            for idx in range(len(self.dataset)):
                futures.append(executor.submit(self._process_item, idx))
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for future in tqdm(futures, total=len(futures), desc="–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞"):
                results.append(future.result())
        
        return results
    
    def _process_item(self, idx):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞"""
        item = self.dataset[idx]
        tokens = item['tokens']
        labels = item['fine_ner_tags']
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        tokens = tokens[:self.max_length-2]
        labels = labels[:self.max_length-2]
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        encoding = self.tokenizer(
            tokens,
            is_split_into_words=True,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
        word_ids = encoding.word_ids()
        previous_word_idx = None
        label_ids = []
        
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                if word_idx < len(labels):
                    label_ids.append(labels[word_idx])
                else:
                    label_ids.append(-100)
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label_ids, dtype=torch.long)
        }
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]

# –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä
class ParallelTrainer:
    def __init__(self, model, config, device):
        self.model = model
        self.config = config
        self.device = device
        self.history = defaultdict(list)
    
    def create_optimizer_scheduler(self, train_loader):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        bert_params = []
        other_params = []
        
        for name, param in self.model.named_parameters():
            if 'bert' in name:
                bert_params.append(param)
            else:
                other_params.append(param)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        optimizer = AdamW([
            {'params': bert_params, 'lr': self.config.learning_rate},
            {'params': other_params, 'lr': self.config.learning_rate * 10}
        ], weight_decay=0.01)
        
        # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
        total_steps = len(train_loader) * self.config.num_epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=self.config.warmup_steps,
            num_training_steps=total_steps
        )
        
        return optimizer, scheduler
    
    def train_epoch(self, train_loader, optimizer, scheduler, epoch):
        """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏"""
        self.model.train()
        total_loss = 0
        progress_bar = tqdm(train_loader, desc=f"–≠–ø–æ—Ö–∞ {epoch+1}/{self.config.num_epochs}")
        
        for batch_idx, batch in enumerate(progress_bar):
            start_time = time.time()
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            optimizer.zero_grad()
            loss, _ = self.model(input_ids, attention_mask, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            
            batch_time = time.time() - start_time
            total_loss += loss.item()
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
            if batch_idx % 10 == 0:
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'batch_time': f'{batch_time:.3f}s'
                })
        
        return total_loss / len(train_loader)
    
    def evaluate(self, loader, id2tag, desc="–û—Ü–µ–Ω–∫–∞"):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
        self.model.eval()
        total_loss = 0
        all_predictions = []
        all_true_labels = []
        
        with torch.no_grad():
            for batch in tqdm(loader, desc=desc, leave=False):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                loss, logits = self.model(input_ids, attention_mask, labels)
                total_loss += loss.item()
                predictions = torch.argmax(logits, dim=-1)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á
                for i in range(len(predictions)):
                    pred_seq = predictions[i].cpu().numpy()
                    true_seq = labels[i].cpu().numpy()
                    mask = attention_mask[i].cpu().numpy()
                    
                    pred_tags = []
                    true_tags = []
                    
                    for j in range(len(pred_seq)):
                        if mask[j] == 1 and true_seq[j] != -100:
                            pred_tags.append(id2tag[pred_seq[j]])
                            true_tags.append(id2tag[true_seq[j]])
                    
                    all_predictions.append(pred_tags)
                    all_true_labels.append(true_tags)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        avg_loss = total_loss / len(loader)
        
        try:
            f1 = f1_score(all_true_labels, all_predictions)
            precision = precision_score(all_true_labels, all_predictions)
            recall = recall_score(all_true_labels, all_predictions)
        except:
            f1, precision, recall = 0.0, 0.0, 0.0
        
        return avg_loss, f1, precision, recall, all_predictions, all_true_labels
    
    def train(self, train_loader, val_loader, id2tag, output_dir):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"""
        optimizer, scheduler = self.create_optimizer_scheduler(train_loader)
        best_f1 = 0
        
        for epoch in range(self.config.num_epochs):
            print(f"\n–≠–ø–æ—Ö–∞ {epoch+1}/{self.config.num_epochs}")
            print("-" * 40)
            
            # –û–±—É—á–µ–Ω–∏–µ
            train_loss = self.train_epoch(train_loader, optimizer, scheduler, epoch)
            self.history['train_loss'].append(train_loss)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss, val_f1, val_precision, val_recall, _, _ = self.evaluate(
                val_loader, id2tag, desc="–í–∞–ª–∏–¥–∞—Ü–∏—è"
            )
            
            self.history['val_loss'].append(val_loss)
            self.history['val_f1'].append(val_f1)
            self.history['val_precision'].append(val_precision)
            self.history['val_recall'].append(val_recall)
            
            print(f"Train Loss: {train_loss:.4f}")
            print(f"Val Loss: {val_loss:.4f}")
            print(f"Val F1: {val_f1:.4f}")
            print(f"Val Precision: {val_precision:.4f}")
            print(f"Val Recall: {val_recall:.4f}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            if val_f1 > best_f1:
                best_f1 = val_f1
                model_path = os.path.join(output_dir, 'best_model_weights.pth')
                torch.save(self.model.state_dict(), model_path)
                print(f"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (F1: {val_f1:.4f})")
        
        return self.history

def set_seed(seed):
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def save_training_plots(history, output_dir):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
    if not history or len(history['train_loss']) == 0:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤")
        return
    
    print("\n–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: Loss
    axes[0, 0].plot(history['train_loss'], label='–û–±—É—á–µ–Ω–∏–µ', marker='o', linewidth=2)
    if 'val_loss' in history and len(history['val_loss']) > 0:
        axes[0, 0].plot(history['val_loss'], label='–í–∞–ª–∏–¥–∞—Ü–∏—è', marker='s', linewidth=2)
    axes[0, 0].set_xlabel('–≠–ø–æ—Ö–∞', fontsize=12)
    axes[0, 0].set_ylabel('Loss', fontsize=12)
    axes[0, 0].set_title('–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å', fontsize=14, fontweight='bold')
    axes[0, 0].legend(fontsize=11)
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].tick_params(axis='both', which='major', labelsize=10)
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: F1-score
    axes[0, 1].plot(history['val_f1'], label='F1-score', color='green', 
                   marker='o', linewidth=2, markersize=8)
    axes[0, 1].set_xlabel('–≠–ø–æ—Ö–∞', fontsize=12)
    axes[0, 1].set_ylabel('F1-score', fontsize=12)
    axes[0, 1].set_title('F1-score –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏', fontsize=14, fontweight='bold')
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].tick_params(axis='both', which='major', labelsize=10)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Ç–æ—á–∫–∏
    for i, val in enumerate(history['val_f1']):
        axes[0, 1].text(i, val + 0.01, f'{val:.3f}', ha='center', va='bottom', fontsize=9)
    
    # –ì—Ä–∞—Ñ–∏–∫ 3: Precision –∏ Recall
    axes[1, 0].plot(history['val_precision'], label='Precision', color='orange', 
                   marker='o', linewidth=2, markersize=8)
    axes[1, 0].plot(history['val_recall'], label='Recall', color='red', 
                   marker='s', linewidth=2, markersize=8)
    axes[1, 0].set_xlabel('–≠–ø–æ—Ö–∞', fontsize=12)
    axes[1, 0].set_ylabel('Score', fontsize=12)
    axes[1, 0].set_title('Precision –∏ Recall', fontsize=14, fontweight='bold')
    axes[1, 0].legend(fontsize=11)
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].tick_params(axis='both', which='major', labelsize=10)
    
    # –ì—Ä–∞—Ñ–∏–∫ 4: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫
    epochs = range(1, len(history['val_f1']) + 1)
    axes[1, 1].plot(epochs, history['val_f1'], label='F1', color='green', marker='o', linewidth=2)
    axes[1, 1].plot(epochs, history['val_precision'], label='Precision', color='orange', marker='s', linewidth=2)
    axes[1, 1].plot(epochs, history['val_recall'], label='Recall', color='red', marker='^', linewidth=2)
    axes[1, 1].set_xlabel('–≠–ø–æ—Ö–∞', fontsize=12)
    axes[1, 1].set_ylabel('Score', fontsize=12)
    axes[1, 1].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫', fontsize=14, fontweight='bold')
    axes[1, 1].legend(fontsize=11)
    axes[1, 1].grid(True, alpha=0.3)
    axes[1, 1].tick_params(axis='both', which='major', labelsize=10)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ layout
    plt.tight_layout(pad=3.0)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –≤—ã—Å–æ–∫–∏–º DPI
    save_path = os.path.join(output_dir, 'training_history.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close(fig)  # –í–∞–∂–Ω–æ: –∑–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∏–≥—É—Ä—É
    
    print(f"‚úì –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path}")
    
    # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ loss
    fig2, ax2 = plt.subplots(figsize=(10, 6))
    ax2.plot(history['train_loss'], label='–û–±—É—á–µ–Ω–∏–µ', marker='o', linewidth=3, markersize=10)
    if 'val_loss' in history and len(history['val_loss']) > 0:
        ax2.plot(history['val_loss'], label='–í–∞–ª–∏–¥–∞—Ü–∏—è', marker='s', linewidth=3, markersize=10)
    ax2.set_xlabel('–≠–ø–æ—Ö–∞', fontsize=14)
    ax2.set_ylabel('Loss', fontsize=14)
    ax2.set_title('–î–∏–Ω–∞–º–∏–∫–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å', fontsize=16, fontweight='bold')
    ax2.legend(fontsize=12)
    ax2.grid(True, alpha=0.3)
    ax2.tick_params(axis='both', which='major', labelsize=12)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Ç–æ—á–∫–∏
    for i, val in enumerate(history['train_loss']):
        ax2.text(i, val + 0.01, f'{val:.3f}', ha='center', va='bottom', fontsize=10)
    
    save_path2 = os.path.join(output_dir, 'loss_history.png')
    plt.tight_layout()
    plt.savefig(save_path2, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close(fig2)
    
    print(f"‚úì –ì—Ä–∞—Ñ–∏–∫ loss —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path2}")

def main():
    # –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    parser = argparse.ArgumentParser(description='–ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ NER')
    parser.add_argument('--debug', action='store_true', default=True)
    parser.add_argument('--epochs', type=int, default=2)
    parser.add_argument('--batch_size', type=int, default=8)
    parser.add_argument('--workers', type=int, default=2)
    parser.add_argument('--model', type=str, default="bert-base-uncased")
    parser.add_argument('--lr', type=float, default=2e-5)
    parser.add_argument('--max_samples', type=int, default=2000)
    parser.add_argument('--max_length', type=int, default=128)
    
    args = parser.parse_args()
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    class Config:
        def __init__(self, args):
            self.model_name = args.model
            self.max_length = args.max_length
            self.batch_size = args.batch_size
            self.num_epochs = args.epochs
            self.learning_rate = args.lr
            self.dropout_prob = 0.1
            self.seed = 42
            self.warmup_steps = 100
            self.output_dir = "ner_results"
            
            self.num_workers = args.workers
            
            self.debug = args.debug
            self.max_samples = args.max_samples
    
    config = Config(args)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞
    set_seed(config.seed)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(config.output_dir, timestamp)
    os.makedirs(output_dir, exist_ok=True)
    
    print("="*60)
    print("–ú–ù–û–ì–û–ü–û–¢–û–ß–ù–ê–Ø –°–ò–°–¢–ï–ú–ê NER –î–õ–Ø FEW-NERD")
    print("="*60)
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print(f"–ú–æ–¥–µ–ª—å: {config.model_name}")
    print(f"–í–æ—Ä–∫–µ—Ä–æ–≤: {config.num_workers}")
    print(f"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {config.batch_size}")
    print(f"–≠–ø–æ—Ö–∏: {config.num_epochs}")
    print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {output_dir}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\n–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ Few-NERD...")
    try:
        dataset = load_dataset("DFKI-SLT/few-nerd", "supervised")
    except:
        dataset = load_dataset("DFKI-SLT/few-nerd")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–≥–∞—Ö
    tag_info = dataset['train'].features['fine_ner_tags'].feature.names
    id2tag = {i: tag for i, tag in enumerate(tag_info)}
    tag2id = {tag: i for i, tag in enumerate(tag_info)}
    
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–≥–æ–≤: {len(tag_info)}")
    print(f"–ü—Ä–∏–º–µ—Ä—ã —Ç–µ–≥–æ–≤: {list(tag_info[:10])}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–≥–∞—Ö
    with open(os.path.join(output_dir, 'tag_info.json'), 'w') as f:
        json.dump({'tag2id': tag2id, 'id2tag': id2tag}, f, indent=2)
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    print("\n–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    
    # –î–∞—Ç–∞—Å–µ—Ç—ã
    print("\n–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...")
    
    train_dataset = FastNERDataset(
        dataset['train'], tokenizer, config.max_length, 
        num_workers=config.num_workers, debug=config.debug, max_samples=config.max_samples
    )
    val_dataset = FastNERDataset(
        dataset['validation'], tokenizer, config.max_length,
        num_workers=config.num_workers, debug=config.debug, max_samples=config.max_samples // 5
    )
    test_dataset = FastNERDataset(
        dataset['test'], tokenizer, config.max_length,
        num_workers=config.num_workers, debug=config.debug, max_samples=config.max_samples // 10
    )
    
    # DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers
    )
    
    print(f"–û–±—É—á–∞—é—â–∏—Ö –±–∞—Ç—á–µ–π: {len(train_loader)}")
    print(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –±–∞—Ç—á–µ–π: {len(val_loader)}")
    print(f"–¢–µ—Å—Ç–æ–≤—ã—Ö –±–∞—Ç—á–µ–π: {len(test_loader)}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print(f"\n–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    model = SimpleBertForNER(
        config.model_name,
        len(tag_info),
        dropout_prob=config.dropout_prob
    )
    
    model.to(device)
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"–í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
    print(f"–û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("\n" + "="*50)
    print("–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")
    print("="*50)
    
    trainer = ParallelTrainer(model, config, device)
    start_time = time.time()
    history = trainer.train(train_loader, val_loader, id2tag, output_dir)
    training_time = time.time() - start_time
    
    print(f"\n–û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time:.1f} —Å–µ–∫—É–Ω–¥")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    print("\n" + "="*50)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï")
    print("="*50)
    
    test_start = time.time()
    test_loss, test_f1, test_precision, test_recall, test_preds, test_true = trainer.evaluate(
        test_loader, id2tag, desc="–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"
    )
    test_time = time.time() - test_start
    
    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ:")
    print(f"  –í—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {test_time:.1f} —Å–µ–∫—É–Ω–¥")
    print(f"  Loss:      {test_loss:.4f}")
    print(f"  F1-score:  {test_f1:.4f}")
    print(f"  Precision: {test_precision:.4f}")
    print(f"  Recall:    {test_recall:.4f}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    try:
        report = classification_report(test_true, test_preds, digits=4)
        print(f"\n–î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:\n{report}")
    except:
        report = "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç"
        print("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    save_training_plots(history, output_dir)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "="*50)
    print("–°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("="*50)
    
    results = {
        'config': {
            'model': config.model_name,
            'epochs': config.num_epochs,
            'batch_size': config.batch_size,
            'learning_rate': config.learning_rate,
            'workers': config.num_workers,
            'max_length': config.max_length
        },
        'results': {
            'test_f1': float(test_f1),
            'test_precision': float(test_precision),
            'test_recall': float(test_recall),
            'test_loss': float(test_loss),
            'training_time': float(training_time),
            'test_time': float(test_time)
        },
        'history': dict(history)
    }
    
    with open(os.path.join(output_dir, 'results.json'), 'w') as f:
        json.dump(results, f, indent=2)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    with open(os.path.join(output_dir, 'test_report.txt'), 'w') as f:
        f.write("–ú–ù–û–ì–û–ü–û–¢–û–ß–ù–ê–Ø –°–ò–°–¢–ï–ú–ê NER - –†–ï–ó–£–õ–¨–¢–ê–¢–´\n")
        f.write("="*60 + "\n\n")
        f.write("–†–ï–ó–£–õ–¨–¢–ê–¢–´:\n")
        f.write(f"  F1-score:         {test_f1:.4f}\n")
        f.write(f"  Precision:        {test_precision:.4f}\n")
        f.write(f"  Recall:           {test_recall:.4f}\n")
        f.write(f"  Loss:             {test_loss:.4f}\n\n")
        f.write("–î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢:\n")
        f.write(report)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_save_path = os.path.join(output_dir, 'model')
    os.makedirs(model_save_path, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
    torch.save(model.state_dict(), os.path.join(model_save_path, 'model_weights.pth'))
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
    model_config = {
        'model_name': config.model_name,
        'num_tags': len(tag_info),
        'dropout_prob': config.dropout_prob
    }
    
    with open(os.path.join(model_save_path, 'config.json'), 'w') as f:
        json.dump(model_config, f, indent=2)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer.save_pretrained(model_save_path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–≥–∞—Ö
    with open(os.path.join(model_save_path, 'tags.json'), 'w') as f:
        json.dump({'id2tag': id2tag, 'tag2id': tag2id}, f, indent=2)
    
    print(f"\n‚úì –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}")
    print(f"‚úì –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {model_save_path}")
    
    # –°–æ–∑–¥–∞–µ–º HTML –æ—Ç—á–µ—Ç —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏
    create_html_report(output_dir, results, history)
    
    print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")

def create_html_report(output_dir, results, history):
    """–°–æ–∑–¥–∞–Ω–∏–µ HTML –æ—Ç—á–µ—Ç–∞ —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏"""
    html_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>–û—Ç—á–µ—Ç –ø–æ –æ–±—É—á–µ–Ω–∏—é NER –º–æ–¥–µ–ª–∏</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 40px; }}
            h1 {{ color: #333; }}
            h2 {{ color: #555; margin-top: 30px; }}
            .container {{ max-width: 1200px; margin: 0 auto; }}
            .card {{ background: #f9f9f9; padding: 20px; margin: 20px 0; border-radius: 10px; }}
            .results {{ display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; }}
            .result-item {{ background: white; padding: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
            .metric {{ font-size: 24px; font-weight: bold; color: #2c3e50; }}
            .metric-label {{ color: #7f8c8d; font-size: 14px; }}
            .images {{ display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin-top: 30px; }}
            img {{ max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.2); }}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>üìä –û—Ç—á–µ—Ç –ø–æ –æ–±—É—á–µ–Ω–∏—é NER –º–æ–¥–µ–ª–∏</h1>
            <p>–î–∞—Ç–∞: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
            
            <div class="card">
                <h2>–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è</h2>
                <ul>
                    <li><strong>–ú–æ–¥–µ–ª—å:</strong> {results['config']['model']}</li>
                    <li><strong>–≠–ø–æ—Ö–∏:</strong> {results['config']['epochs']}</li>
                    <li><strong>–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞:</strong> {results['config']['batch_size']}</li>
                    <li><strong>Learning rate:</strong> {results['config']['learning_rate']}</li>
                    <li><strong>Workers:</strong> {results['config']['workers']}</li>
                </ul>
            </div>
            
            <div class="card">
                <h2>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã</h2>
                <div class="results">
                    <div class="result-item">
                        <div class="metric">{results['results']['test_f1']:.4f}</div>
                        <div class="metric-label">F1-Score</div>
                    </div>
                    <div class="result-item">
                        <div class="metric">{results['results']['test_precision']:.4f}</div>
                        <div class="metric-label">Precision</div>
                    </div>
                    <div class="result-item">
                        <div class="metric">{results['results']['test_recall']:.4f}</div>
                        <div class="metric-label">Recall</div>
                    </div>
                    <div class="result-item">
                        <div class="metric">{results['results']['test_loss']:.4f}</div>
                        <div class="metric-label">Loss</div>
                    </div>
                </div>
            </div>
            
            <div class="card">
                <h2>–ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è</h2>
                <div class="images">
                    <div>
                        <h3>–î–∏–Ω–∞–º–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è</h3>
                        <img src="training_history.png" alt="Training History">
                    </div>
                    <div>
                        <h3>–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å</h3>
                        <img src="loss_history.png" alt="Loss History">
                    </div>
                </div>
            </div>
            
            <div class="card">
                <h2>–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å</h2>
                <ul>
                    <li><strong>–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:</strong> {results['results']['training_time']:.1f} —Å–µ–∫—É–Ω–¥</li>
                    <li><strong>–í—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:</strong> {results['results']['test_time']:.1f} —Å–µ–∫—É–Ω–¥</li>
                    <li><strong>–û–±—â–µ–µ –≤—Ä–µ–º—è:</strong> {results['results']['training_time'] + results['results']['test_time']:.1f} —Å–µ–∫—É–Ω–¥</li>
                </ul>
            </div>
        </div>
    </body>
    </html>
    """
    
    with open(os.path.join(output_dir, 'report.html'), 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"‚úì HTML –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {os.path.join(output_dir, 'report.html')}")

if __name__ == "__main__":
    main()